{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tonimono/AnimateDiff/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at /home/tonimono/AnimateDiff/models/StableDiffusion/ACertainThing were not used when initializing CLIPTextModel: ['text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.11.layer_norm2.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded temporal unet's pretrained weights from /home/tonimono/AnimateDiff/models/StableDiffusion/ACertainThing/unet ...\n",
      "### missing keys: 560; \n",
      "### unexpected keys: 0;\n",
      "### Temporal Module Parameters: 417.1376 M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tonimono/AnimateDiff/animatediff/pipelines/pipeline_animation_inpaint.py:73: FutureWarning: The configuration file of this scheduler: DDIMScheduler {\n",
      "  \"_class_name\": \"DDIMScheduler\",\n",
      "  \"_diffusers_version\": \"0.11.1\",\n",
      "  \"beta_end\": 0.012,\n",
      "  \"beta_schedule\": \"linear\",\n",
      "  \"beta_start\": 0.00085,\n",
      "  \"clip_sample\": true,\n",
      "  \"num_train_timesteps\": 1000,\n",
      "  \"prediction_type\": \"epsilon\",\n",
      "  \"set_alpha_to_one\": true,\n",
      "  \"steps_offset\": 0,\n",
      "  \"trained_betas\": null\n",
      "}\n",
      " is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\n",
      "  deprecate(\"steps_offset!=1\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "/home/tonimono/AnimateDiff/animatediff/pipelines/pipeline_animation_inpaint.py:86: FutureWarning: The configuration file of this scheduler: DDIMScheduler {\n",
      "  \"_class_name\": \"DDIMScheduler\",\n",
      "  \"_diffusers_version\": \"0.11.1\",\n",
      "  \"beta_end\": 0.012,\n",
      "  \"beta_schedule\": \"linear\",\n",
      "  \"beta_start\": 0.00085,\n",
      "  \"clip_sample\": true,\n",
      "  \"num_train_timesteps\": 1000,\n",
      "  \"prediction_type\": \"epsilon\",\n",
      "  \"set_alpha_to_one\": true,\n",
      "  \"steps_offset\": 1,\n",
      "  \"trained_betas\": null\n",
      "}\n",
      " has not set the configuration `clip_sample`. `clip_sample` should be set to False in the configuration file. Please make sure to update the config accordingly as not setting `clip_sample` in the config might lead to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file\n",
      "  deprecate(\"clip_sample not set\", \"1.0.0\", deprecation_message, standard_warn=False)\n",
      "/home/tonimono/AnimateDiff/animatediff/pipelines/pipeline_animation_inpaint.py:107: FutureWarning: The configuration file of the unet has set the default `sample_size` to smaller than 64 which seems highly unlikely. If your checkpoint is a fine-tuned version of any of the following: \n",
      "- CompVis/stable-diffusion-v1-4 \n",
      "- CompVis/stable-diffusion-v1-3 \n",
      "- CompVis/stable-diffusion-v1-2 \n",
      "- CompVis/stable-diffusion-v1-1 \n",
      "- runwayml/stable-diffusion-v1-5 \n",
      "- runwayml/stable-diffusion-inpainting \n",
      " you should change 'sample_size' to 64 in the configuration file. Please make sure to update the config accordingly as leaving `sample_size=32` in the config might lead to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `unet/config.json` file\n",
      "  deprecate(\"sample_size<64\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from animatediff.pipelines.pipeline_animation_inpaint import AnimationInpaintPipeline\n",
    "\n",
    "from diffusers import DDIMScheduler, EulerDiscreteScheduler, PNDMScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL\n",
    "from animatediff.models.unet import UNet3DConditionModel\n",
    "\n",
    "stable_diffusion_model_path = os.path.join(os.getcwd(), \"models\", \"StableDiffusion\", \"ACertainThing\")\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(stable_diffusion_model_path, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(stable_diffusion_model_path, subfolder=\"text_encoder\").cuda()\n",
    "vae = AutoencoderKL.from_pretrained(stable_diffusion_model_path, subfolder=\"vae\").cuda()\n",
    "unet = UNet3DConditionModel.from_pretrained_2d(stable_diffusion_model_path, subfolder=\"unet\", unet_additional_kwargs={\n",
    "    \"unet_use_cross_frame_attention\": False,\n",
    "    \"unet_use_temporal_attention\": False,\n",
    "    \"use_motion_module\": True,\n",
    "    \"motion_module_resolutions\": [1, 2, 4, 8],\n",
    "    \"motion_module_mid_block\": False,\n",
    "    \"motion_module_decoder_only\": False,\n",
    "    \"motion_module_type\": \"Vanilla\",\n",
    "    \"motion_module_kwargs\": {\n",
    "        \"num_attention_heads\": 8,\n",
    "        \"num_transformer_block\": 1,\n",
    "        \"attention_block_types\": [\"Temporal_Self\", \"Temporal_Self\"],\n",
    "        \"temporal_position_encoding\": True,\n",
    "        \"temporal_position_encoding_max_len\": 24,\n",
    "        \"temporal_attention_dim_div\": 1\n",
    "    }\n",
    "}).cuda()\n",
    "motion_module_path = os.path.join(os.getcwd(), \"models\", \"Motion_Module\", \"mm_sd_v14.ckpt\")\n",
    "motion_module_state_dict = torch.load(motion_module_path, map_location=\"cpu\")\n",
    "missing, unexpected = unet.load_state_dict(motion_module_state_dict, strict=False)\n",
    "assert len(unexpected) == 0\n",
    "unet.enable_xformers_memory_efficient_attention()\n",
    "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"linear\")\n",
    "\n",
    "pipeline = AnimationInpaintPipeline(vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, unet=unet, scheduler=scheduler).to(\"cuda\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "\n",
    "prompt = \"best quality, masterpiece, 1girl, looking at viewer\"\n",
    "negative_prompt = \"\"\n",
    "num_inference_steps = 25\n",
    "guidance_scale = 7.5\n",
    "width = 512\n",
    "height = 512\n",
    "video_length = 8\n",
    "seed = 1\n",
    "keyframes = {\n",
    "    0: PIL.Image.open(\"images/0.jpeg\"),\n",
    "    7: PIL.Image.open(\"images/15.jpeg\"),\n",
    "}\n",
    "add_predicted_noise = False\n",
    "do_reconstruction_guidance = True\n",
    "reconstruction_guidance_scale = 100.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [02:11<00:00,  5.27s/it]\n",
      "100%|██████████| 8/8 [00:00<00:00, 14.92it/s]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from animatediff.utils.util import save_videos_grid\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "sample = pipeline(\n",
    "    prompt = prompt,\n",
    "    negative_prompt = negative_prompt,\n",
    "    num_inference_steps = num_inference_steps,\n",
    "    guidance_scale = guidance_scale,\n",
    "    width = width,\n",
    "    height = height,\n",
    "    video_length = video_length,\n",
    "    keyframes = keyframes,\n",
    "    add_predicted_noise=add_predicted_noise,\n",
    "    do_reconstruction_guidance=do_reconstruction_guidance,\n",
    "    reconstruction_guidance_scale=reconstruction_guidance_scale\n",
    ").videos\n",
    "\n",
    "savedir = os.path.join(os.getcwd(), \"samples\", datetime.now().strftime(\"Gradio-%Y-%m-%dT%H-%M-%S\"))\n",
    "savedir_sample = os.path.join(savedir, \"sample\")\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "save_sample_path = os.path.join(savedir_sample, f\"{str(int(time.time()))}.mp4\")\n",
    "save_videos_grid(sample, save_sample_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
