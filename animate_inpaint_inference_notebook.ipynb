{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from animatediff.pipelines.pipeline_animation_inpaint import AnimationInpaintPipeline\n",
    "\n",
    "from diffusers import DDIMScheduler, EulerDiscreteScheduler, PNDMScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL\n",
    "from animatediff.models.unet import UNet3DConditionModel\n",
    "\n",
    "stable_diffusion_model_path = os.path.join(os.getcwd(), \"models\", \"StableDiffusion\", \"ACertainThing\")\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(stable_diffusion_model_path, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(stable_diffusion_model_path, subfolder=\"text_encoder\").cuda()\n",
    "vae = AutoencoderKL.from_pretrained(stable_diffusion_model_path, subfolder=\"vae\").cuda()\n",
    "unet = UNet3DConditionModel.from_pretrained_2d(stable_diffusion_model_path, subfolder=\"unet\", unet_additional_kwargs={\n",
    "    \"unet_use_cross_frame_attention\": False,\n",
    "    \"unet_use_temporal_attention\": False,\n",
    "    \"use_motion_module\": True,\n",
    "    \"motion_module_resolutions\": [1, 2, 4, 8],\n",
    "    \"motion_module_mid_block\": False,\n",
    "    \"motion_module_decoder_only\": False,\n",
    "    \"motion_module_type\": \"Vanilla\",\n",
    "    \"motion_module_kwargs\": {\n",
    "        \"num_attention_heads\": 8,\n",
    "        \"num_transformer_block\": 1,\n",
    "        \"attention_block_types\": [\"Temporal_Self\", \"Temporal_Self\"],\n",
    "        \"temporal_position_encoding\": True,\n",
    "        \"temporal_position_encoding_max_len\": 24,\n",
    "        \"temporal_attention_dim_div\": 1\n",
    "    }\n",
    "}).cuda()\n",
    "motion_module_path = os.path.join(os.getcwd(), \"models\", \"Motion_Module\", \"mm_sd_v14.ckpt\")\n",
    "motion_module_state_dict = torch.load(motion_module_path, map_location=\"cpu\")\n",
    "missing, unexpected = unet.load_state_dict(motion_module_state_dict, strict=False)\n",
    "assert len(unexpected) == 0\n",
    "unet.enable_xformers_memory_efficient_attention()\n",
    "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"linear\")\n",
    "\n",
    "self = AnimationInpaintPipeline(vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, unet=unet, scheduler=scheduler).to(\"cuda\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "\n",
    "prompt = \"best quality, masterpiece, 1girl, looking at viewer, hatsune miku\"\n",
    "negative_prompt = \"\"\n",
    "num_inference_steps = 25\n",
    "guidance_scale = 7.5\n",
    "width = 512\n",
    "height = 512\n",
    "video_length = 16\n",
    "seed = 1\n",
    "keyframes = {\n",
    "    0: PIL.Image.open(\"images/0.jpeg\"),\n",
    "    15: PIL.Image.open(\"images/15.jpeg\")\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from animatediff.utils.util import save_videos_grid\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "prompt = prompt\n",
    "negative_prompt = negative_prompt\n",
    "num_inference_steps = num_inference_steps\n",
    "guidance_scale = guidance_scale\n",
    "width = width\n",
    "height = height\n",
    "video_length = video_length\n",
    "keyframes = keyframes\n",
    "add_predicted_noise = False\n",
    "do_reconstruction_guidance = True\n",
    "reconstruction_guidance_scale = 3.0\n",
    "\n",
    "num_videos_per_prompt = 1\n",
    "eta = 0.0\n",
    "generator = None\n",
    "latents = None\n",
    "output_type = \"tensor\"\n",
    "return_dict = True\n",
    "callback = None\n",
    "callback_steps = 1\n",
    "\n",
    "self.unet.requires_grad_(False)\n",
    "self.vae.requires_grad_(False)\n",
    "self.text_encoder.text_model.encoder.requires_grad_(False)\n",
    "self.text_encoder.text_model.final_layer_norm.requires_grad_(False)\n",
    "self.text_encoder.text_model.embeddings.position_embedding.requires_grad_(False)\n",
    "\n",
    "# Default height and width to unet\n",
    "height = height or self.unet.config.sample_size * self.vae_scale_factor\n",
    "width = width or self.unet.config.sample_size * self.vae_scale_factor\n",
    "# Check inputs. Raise error if not correct\n",
    "self.check_inputs(prompt, height, width, callback_steps)\n",
    "# Define call parameters\n",
    "# batch_size = 1 if isinstance(prompt, str) else len(prompt)\n",
    "batch_size = 1\n",
    "if latents is not None:\n",
    "    batch_size = latents.shape[0]\n",
    "if isinstance(prompt, list):\n",
    "    batch_size = len(prompt)\n",
    "device = self._execution_device\n",
    "# here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
    "# of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
    "# corresponds to doing no classifier free guidance.\n",
    "do_classifier_free_guidance = guidance_scale > 1.0\n",
    "# Encode input prompt\n",
    "prompt = prompt if isinstance(prompt, list) else [prompt] * batch_size\n",
    "if negative_prompt is not None:\n",
    "    negative_prompt = negative_prompt if isinstance(negative_prompt, list) else [negative_prompt] * batch_size \n",
    "text_embeddings = self._encode_prompt(\n",
    "    prompt, device, num_videos_per_prompt, do_classifier_free_guidance, negative_prompt\n",
    ")\n",
    "# Prepare timesteps\n",
    "self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "timesteps = self.scheduler.timesteps\n",
    "# Prepare latent variables\n",
    "num_channels_latents = self.unet.in_channels\n",
    "# latents shape is [Batch, Channel, Frame, Height, Width]\n",
    "latents = self.prepare_latents(\n",
    "    batch_size * num_videos_per_prompt,\n",
    "    num_channels_latents,\n",
    "    video_length,\n",
    "    height,\n",
    "    width,\n",
    "    text_embeddings.dtype,\n",
    "    device,\n",
    "    generator,\n",
    "    latents,\n",
    ")\n",
    "latents_dtype = latents.dtype\n",
    "# Prepare mask for keyframes\n",
    "# mask shape is [Batch, Channel, Frame, Height, Width]\n",
    "# and is 1 for [Batch, Channel, keyframe, Height, Width] and 0 for others\n",
    "zeros = torch.zeros(\n",
    "    (\n",
    "        batch_size * num_videos_per_prompt,\n",
    "        num_channels_latents,\n",
    "        video_length,\n",
    "        int(height / self.vae_scale_factor),\n",
    "        int(width / self.vae_scale_factor)\n",
    "    ),\n",
    "    device=device\n",
    ")\n",
    "mask = zeros.clone()\n",
    "for keyframe_idx in keyframes.keys():\n",
    "    mask[:, :, keyframe_idx, :, :] = 1\n",
    "# Prepare image latents\n",
    "# preprocess all keyframes\n",
    "keyframes_latents = zeros.clone()\n",
    "keyframes_init_latents_orig = zeros.clone()\n",
    "keyframes_noise = zeros.clone()\n",
    "latent_timestep = timesteps[:1].repeat(batch_size * num_videos_per_prompt)\n",
    "for keyframe_idx, keyframe in keyframes.items():\n",
    "    if isinstance(keyframe, PIL.Image.Image):\n",
    "        keyframe = self.preprocess_image(keyframe)\n",
    "    keyframe_latents, keyframe_init_latents_orig, keyframe_noise = self.prepare_image_latents(keyframe, latent_timestep, batch_size * num_videos_per_prompt, text_embeddings.dtype, device, generator)\n",
    "    keyframes_latents[:, :, keyframe_idx, :, :] = keyframe_latents\n",
    "    keyframes_init_latents_orig[:, :, keyframe_idx, :, :] = keyframe_init_latents_orig\n",
    "    keyframes_noise[:, :, keyframe_idx, :, :] = keyframe_noise\n",
    "\n",
    "latents = keyframes_latents * mask + latents * (1 - mask)\n",
    "# Prepare extra step kwargs.\n",
    "extra_step_kwargs = self.prepare_extra_step_kwargs(generator, eta)\n",
    "# Denoising loop\n",
    "num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n",
    "with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "    for i, t in enumerate(timesteps):\n",
    "        # expand the latents if we are doing classifier free guidance\n",
    "        latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "        latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "        # predict the noise residual\n",
    "        noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample.to(dtype=latents_dtype)\n",
    "        # noise_pred = []\n",
    "        # import pdb\n",
    "        # pdb.set_trace()\n",
    "        # for batch_idx in range(latent_model_input.shape[0]):\n",
    "        #     noise_pred_single = self.unet(latent_model_input[batch_idx:batch_idx+1], t, encoder_hidden_states=text_embeddings[batch_idx:batch_idx+1]).sample.to(dtype=latents_dtype)\n",
    "        #     noise_pred.append(noise_pred_single)\n",
    "        # noise_pred = torch.cat(noise_pred)\n",
    "        # perform guidance\n",
    "        if do_classifier_free_guidance:\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "        # compute the previous noisy sample x_t -> x_t-1\n",
    "        latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs).prev_sample\n",
    "        # reconstruction guidance\n",
    "        if do_reconstruction_guidance:\n",
    "            pass\n",
    "        # masking\n",
    "        if add_predicted_noise:\n",
    "            init_latents_proper = self.scheduler.add_noise(\n",
    "                keyframes_init_latents_orig,\n",
    "                noise_pred_uncond,\n",
    "                torch.tensor([t])\n",
    "            )\n",
    "        else:\n",
    "            init_latents_proper = self.scheduler.add_noise(\n",
    "                keyframes_init_latents_orig,\n",
    "                keyframes_noise,\n",
    "                torch.tensor([t])\n",
    "            )\n",
    "        latents = init_latents_proper * mask + latents * (1 - mask)\n",
    "        # call the callback, if provided\n",
    "        if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
    "            progress_bar.update()\n",
    "            if callback is not None and i % callback_steps == 0:\n",
    "                callback(i, t, latents)\n",
    "# Post-processing\n",
    "video = self.decode_latents(latents)\n",
    "# Convert to tensor\n",
    "if output_type == \"tensor\":\n",
    "    video = torch.from_numpy(video)\n",
    "if not return_dict:\n",
    "    pass\n",
    "\n",
    "savedir = os.path.join(os.getcwd(), \"sample\", datetime.now().strftime(\"Gradio-%Y-%m-%dT%H-%M-%S\"))\n",
    "savedir_sample = os.path.join(savedir, \"sample\")\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "save_sample_path = os.path.join(savedir_sample, f\"{str(int(time.time()))}.mp4\")\n",
    "save_videos_grid(video, save_sample_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
