{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tonimono/AnimateDiff/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at /home/tonimono/AnimateDiff/models/StableDiffusion/ACertainThing were not used when initializing CLIPTextModel: ['text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded temporal unet's pretrained weights from /home/tonimono/AnimateDiff/models/StableDiffusion/ACertainThing/unet ...\n",
      "### missing keys: 560; \n",
      "### unexpected keys: 0;\n",
      "### Temporal Module Parameters: 417.1376 M\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from animatediff.pipelines.pipeline_animation_inpaint import AnimationInpaintPipeline\n",
    "\n",
    "from diffusers import DDIMScheduler, EulerDiscreteScheduler, PNDMScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL\n",
    "from animatediff.models.unet import UNet3DConditionModel\n",
    "\n",
    "stable_diffusion_model_path = os.path.join(os.getcwd(), \"models\", \"StableDiffusion\", \"ACertainThing\")\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(stable_diffusion_model_path, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(stable_diffusion_model_path, subfolder=\"text_encoder\").cuda()\n",
    "vae = AutoencoderKL.from_pretrained(stable_diffusion_model_path, subfolder=\"vae\").cuda()\n",
    "unet = UNet3DConditionModel.from_pretrained_2d(stable_diffusion_model_path, subfolder=\"unet\", unet_additional_kwargs={\n",
    "    \"unet_use_cross_frame_attention\": False,\n",
    "    \"unet_use_temporal_attention\": False,\n",
    "    \"use_motion_module\": True,\n",
    "    \"motion_module_resolutions\": [1, 2, 4, 8],\n",
    "    \"motion_module_mid_block\": False,\n",
    "    \"motion_module_decoder_only\": False,\n",
    "    \"motion_module_type\": \"Vanilla\",\n",
    "    \"motion_module_kwargs\": {\n",
    "        \"num_attention_heads\": 8,\n",
    "        \"num_transformer_block\": 1,\n",
    "        \"attention_block_types\": [\"Temporal_Self\", \"Temporal_Self\"],\n",
    "        \"temporal_position_encoding\": True,\n",
    "        \"temporal_position_encoding_max_len\": 24,\n",
    "        \"temporal_attention_dim_div\": 1\n",
    "    }\n",
    "}).cuda()\n",
    "motion_module_path = os.path.join(os.getcwd(), \"models\", \"Motion_Module\", \"mm_sd_v14.ckpt\")\n",
    "motion_module_state_dict = torch.load(motion_module_path, map_location=\"cpu\")\n",
    "missing, unexpected = unet.load_state_dict(motion_module_state_dict, strict=False)\n",
    "assert len(unexpected) == 0\n",
    "unet.enable_xformers_memory_efficient_attention()\n",
    "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"linear\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "\n",
    "prompt = \"best quality, masterpiece, 1girl, looking at viewer, blurry background, upper body, contemporary, dress\"\n",
    "negative_prompt = \"\"\n",
    "num_inference_steps = 25\n",
    "guidance_scale = 7.5\n",
    "width = 512\n",
    "height = 512\n",
    "video_length = 16\n",
    "seed = 1\n",
    "start_image = PIL.Image.open(os.path.join(os.getcwd(), \"images\", \"start_image.png\")).convert(\"RGB\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import inspect\n",
    "import numpy as np\n",
    "\n",
    "vae_scale_factor = 2 ** (len(vae.config.block_out_channels) - 1)\n",
    "\n",
    "def preprocess_image(image):\n",
    "    w, h = image.size\n",
    "    w, h = map(lambda x: x - x % 32, (w, h))  # resize to integer multiple of 32\n",
    "    image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n",
    "    image = np.array(image).astype(np.float32) / 255.0\n",
    "    image = image[None].transpose(0, 3, 1, 2)\n",
    "    image = torch.from_numpy(image)\n",
    "    return 2.0 * image - 1.0\n",
    "\n",
    "def encode_prompt(prompt, device, negative_prompt):\n",
    "    batch_size = len(prompt) if isinstance(prompt, list) else 1\n",
    "    text_inputs = tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    text_input_ids = text_inputs.input_ids\n",
    "    untruncated_ids = tokenizer(prompt, padding=\"longest\", return_tensors=\"pt\").input_ids\n",
    "    if untruncated_ids.shape[-1] >= text_input_ids.shape[-1] and not torch.equal(text_input_ids, untruncated_ids):\n",
    "        removed_text = tokenizer.batch_decode(untruncated_ids[:, tokenizer.model_max_length - 1 : -1])\n",
    "    if hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask:\n",
    "        attention_mask = text_inputs.attention_mask.to(device)\n",
    "    else:\n",
    "        attention_mask = None\n",
    "    text_embeddings = text_encoder(\n",
    "        text_input_ids.to(device),\n",
    "        attention_mask=attention_mask,\n",
    "    )\n",
    "    text_embeddings = text_embeddings[0]\n",
    "    # duplicate text embeddings for each generation per prompt, using mps friendly method\n",
    "    bs_embed, seq_len, _ = text_embeddings.shape\n",
    "    text_embeddings = text_embeddings.view(bs_embed, seq_len, -1)\n",
    "    # get unconditional embeddings for classifier free guidance\n",
    "    uncond_tokens: List[str]\n",
    "    if negative_prompt is None:\n",
    "        uncond_tokens = [\"\"] * batch_size\n",
    "    elif type(prompt) is not type(negative_prompt):\n",
    "        raise TypeError(\n",
    "            f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n",
    "            f\" {type(prompt)}.\"\n",
    "        )\n",
    "    elif isinstance(negative_prompt, str):\n",
    "        uncond_tokens = [negative_prompt]\n",
    "    elif batch_size != len(negative_prompt):\n",
    "        raise ValueError(\n",
    "            f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n",
    "            f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n",
    "            \" the batch size of `prompt`.\"\n",
    "        )\n",
    "    else:\n",
    "        uncond_tokens = negative_prompt\n",
    "    max_length = text_input_ids.shape[-1]\n",
    "    uncond_input = tokenizer(\n",
    "        uncond_tokens,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    if hasattr(text_encoder.config, \"use_attention_mask\") and text_encoder.config.use_attention_mask:\n",
    "        attention_mask = uncond_input.attention_mask.to(device)\n",
    "    else:\n",
    "        attention_mask = None\n",
    "    uncond_embeddings = text_encoder(\n",
    "        uncond_input.input_ids.to(device),\n",
    "        attention_mask=attention_mask,\n",
    "    )\n",
    "    uncond_embeddings = uncond_embeddings[0]\n",
    "    # duplicate unconditional embeddings for each generation per prompt, using mps friendly method\n",
    "    seq_len = uncond_embeddings.shape[1]\n",
    "    uncond_embeddings = uncond_embeddings.view(batch_size, seq_len, -1)\n",
    "    # For classifier free guidance, we need to do two forward passes.\n",
    "    # Here we concatenate the unconditional and text embeddings into a single batch\n",
    "    # to avoid doing two forward passes\n",
    "    text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "    return text_embeddings\n",
    "\n",
    "def prepare_latents(batch_size, num_channels_latents, video_length, height, width, dtype, device):\n",
    "    shape = (batch_size, num_channels_latents, video_length, height // vae_scale_factor, width // vae_scale_factor)\n",
    "    rand_device = device\n",
    "    latents = torch.randn(shape, device=rand_device, dtype=dtype).to(device)\n",
    "    # scale the initial noise by the standard deviation required by the scheduler\n",
    "    latents = latents * scheduler.init_noise_sigma\n",
    "    return latents\n",
    "\n",
    "def prepare_image_latents(image, timestep, batch_size, dtype, device):\n",
    "    image = image.to(device=device, dtype=dtype)\n",
    "    init_latent_dist = vae.encode(image).latent_dist\n",
    "    init_latents = init_latent_dist.sample()\n",
    "    init_latents = 0.18215 * init_latents\n",
    "    init_latents = torch.cat([init_latents] * batch_size, dim=0)\n",
    "    init_latents_orig = init_latents\n",
    "    noise = torch.randn(init_latents.shape, device=device, dtype=dtype)\n",
    "    init_latents = scheduler.add_noise(init_latents, noise, timestep)\n",
    "    latents = init_latents\n",
    "    return latents, init_latents_orig, noise\n",
    "\n",
    "\n",
    "\n",
    "def prepare_extra_step_kwargs(generator, eta):\n",
    "    # prepare extra kwargs for the scheduler step, since not all schedulers have the same signature\n",
    "    # eta (η) is only used with the DDIMScheduler, it will be ignored for other schedulers.\n",
    "    # eta corresponds to η in DDIM paper: https://arxiv.org/abs/2010.02502\n",
    "    # and should be between [0, 1]\n",
    "\n",
    "    accepts_eta = \"eta\" in set(inspect.signature(scheduler.step).parameters.keys())\n",
    "    extra_step_kwargs = {}\n",
    "    if accepts_eta:\n",
    "        extra_step_kwargs[\"eta\"] = eta\n",
    "\n",
    "    # check if the scheduler accepts generator\n",
    "    accepts_generator = \"generator\" in set(inspect.signature(scheduler.step).parameters.keys())\n",
    "    if accepts_generator:\n",
    "        extra_step_kwargs[\"generator\"] = generator\n",
    "    return extra_step_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 16, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35045/1891398937.py:10: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  image = image.resize((w, h), resample=PIL.Image.LANCZOS)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 96, 64])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "if isinstance(prompt, list):\n",
    "    batch_size = len(prompt)\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "prompt = prompt if isinstance(prompt, list) else [prompt] * batch_size\n",
    "if negative_prompt is not None:\n",
    "    negative_prompt = negative_prompt if isinstance(negative_prompt, list) else [negative_prompt] * batch_size \n",
    "text_embeddings = encode_prompt(\n",
    "    prompt, device, negative_prompt\n",
    ")\n",
    "\n",
    "scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "timesteps = scheduler.timesteps\n",
    "\n",
    "num_channels_latents = unet.in_channels\n",
    "\n",
    "latents = prepare_latents(batch_size, num_channels_latents, video_length, height, width, text_embeddings.dtype, device)\n",
    "print(latents.shape)\n",
    "latents_dtype = latents.dtype\n",
    "\n",
    "start_image = preprocess_image(start_image)\n",
    "\n",
    "latent_timestep = timesteps[:1].repeat(batch_size)\n",
    "image_latents, init_latents_orig, noise = prepare_image_latents(start_image, timesteps[0], batch_size, latents_dtype, device)\n",
    "print(image_latents.shape)\n",
    "\n",
    "extra_step_kwargs = {\"eta\": 0.0, \"generator\": None}\n",
    "\n",
    "latent_model_input = torch.cat([latents] * 2)\n",
    "latent_model_input = scheduler.scale_model_input(latent_model_input, timesteps[0])\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from animatediff.utils.util import save_videos_grid\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "savedir = os.path.join(os.getcwd(), \"sample\", datetime.now().strftime(\"Gradio-%Y-%m-%dT%H-%M-%S\"))\n",
    "savedir_sample = os.path.join(savedir, \"sample\")\n",
    "os.makedirs(savedir, exist_ok=True)\n",
    "\n",
    "save_sample_path = os.path.join(savedir_sample, f\"{str(int(time.time()))}.mp4\")\n",
    "save_videos_grid(sample, save_sample_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
